# **Modern Deep Learning Training Pipeline for Image Classification**

A **flexible and extensible PyTorch-based training pipeline** designed for image classification tasks. This project leverages **Hydra for configuration management**, **Weights & Biases for experiment tracking and hyperparameter sweeps**, and **Albumentations for advanced data augmentation**. It demonstrates training various models (Custom CNN, ResNet50, EfficientNetV2-S/M) on the "A Large Scale Fish Dataset".

---

## **ğŸ“œ Table of Contents**
- [Features](#-features)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Dataset Setup](#-dataset-setup)
- [Configuration](#-configuration)
- [Training](#-training)
- [Hyperparameter Sweeps with W&B](#-hyperparameter-sweeps-with-wb)
- [Supported Models](#-supported-models)
- [Key Training Components](#-key-training-components)
- [Results & Checkpoints](#-results--checkpoints)
- [To-Do](#-to-do)
- [License](#-license)
- [Acknowledgments](#-acknowledgments)

---

## **âœ¨ Features**
âœ” **Flexible Model Architecture**  
âœ” **Transfer Learning**  
âœ” **Hydra for Configuration**  
âœ” **Weights & Biases Integration**  
âœ” **Advanced Data Augmentation**  
âœ” **Mixed Precision Training**  
âœ” **Learning Rate Scheduling**  
âœ” **Early Stopping**  
âœ” **Modular Codebase**  
âœ” **Comprehensive Metrics**  
âœ” **Efficient Data Loading**

---

## **ğŸ“‚ Project Structure**

```

mohamedkhayat-modern-training-pipeline/
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ conf/
â”‚   â”œâ”€â”€ config.yaml
â”‚   â”œâ”€â”€ cnnsweep.yaml
â”‚   â”œâ”€â”€ sweep.yaml
â”‚   â””â”€â”€ model/
â”‚       â”œâ”€â”€ cnn.yaml
â”‚       â”œâ”€â”€ efficientnet\_v2\_m.yaml
â”‚       â”œâ”€â”€ efficientnet\_v2\_s.yaml
â”‚       â””â”€â”€ resnet50.yaml
â”œâ”€â”€ data/
â”‚   â””â”€â”€ NA\_Fish\_Dataset/
â”‚       â””â”€â”€ .gitkeep
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dataset.py
â”‚   â”œâ”€â”€ early\_stop.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ model\_factory.py
â”‚   â””â”€â”€ utils.py
â””â”€â”€ checkpoints/

````

---

## **âš¡ Installation**

### **1ï¸âƒ£ Clone the Repository**
```bash
git clone https://github.com/your-username/mohamedkhayat-modern-training-pipeline.git
cd mohamedkhayat-modern-training-pipeline
````

### **2ï¸âƒ£ Set Up a Conda Environment**

```bash
conda create --name fishclf python=3.9 -y
conda activate fishclf
```

### **3ï¸âƒ£ Install Dependencies**

```bash
pip install -r requirements.txt
```

> **Note:** Ensure PyTorch with CUDA support is installed if using GPU. Check the [official PyTorch install guide](https://pytorch.org/get-started/locally/).

---

## **ğŸŸ Dataset Setup**

This pipeline uses the "A Large Scale Fish Dataset" from Kaggle.

1. Download it from [Kaggle](https://www.kaggle.com/datasets/crowww/a-large-scale-fish-dataset).
2. Extract the archive.
3. Move class folders (e.g. `Red Mullet`, `Gilt-Head Bream`, etc.) into `data/NA_Fish_Dataset/`.

**Expected directory layout:**

```
data/
â””â”€â”€ NA_Fish_Dataset/
    â”œâ”€â”€ Black Sea Sprat/
    â”œâ”€â”€ Gilt-Head Bream/
    â””â”€â”€ ...
```

---

## **âš™ï¸ Configuration**

Managed with **Hydra**.

* **Main config:** `conf/config.yaml`
* **Model-specific configs:** `conf/model/*.yaml`
* **Sweep configs:** `conf/sweep.yaml`, `conf/cnnsweep.yaml`

### **Example overrides:**

```bash
# Use custom CNN with dropout
python src/main.py model=cnn batch_size=32 model.dropout=0.25

# Change LR and epochs
python src/main.py lr=0.0005 epochs=30
```

---

## **ğŸ¯ Training**

### **1ï¸âƒ£ Default Training**

```bash
python src/main.py
```

### **2ï¸âƒ£ Disable W\&B Logging**

```bash
python src/main.py logwandb=False
```

### **3ï¸âƒ£ Train with Different Model**

```bash
python src/main.py model=efficientnet_v2_s
python src/main.py model=cnn model.hidden_size=1024 model.dropout=0.4
```

---

## **ğŸ§ª Hyperparameter Sweeps with W\&B**

### **Install/Upgrade W\&B**

```bash
pip install wandb --upgrade
```

### **Login**

```bash
wandb login
```

### **Initialize Sweep**

```bash
# Pretrained models
wandb sweep conf/sweep.yaml

# Custom CNN
wandb sweep conf/cnnsweep.yaml
```

### **Run Sweep Agent**

```bash
wandb agent <YOUR_SWEEP_ID>
```

---

## **ğŸ¤– Supported Models**

| Model               | Description                                               |
| ------------------- | --------------------------------------------------------- |
| `cnn`               | Custom CNN with configurable layers, dropout              |
| `resnet50`          | Torchvision pretrained model with optional layer freezing |
| `efficientnet_v2_s` | Pretrained EfficientNetV2-S                               |
| `efficientnet_v2_m` | Pretrained EfficientNetV2-M                               |

**Freezing layers:** controlled via `startpoint` in config (everything before it is frozen).

---

## **ğŸ§© Key Training Components**

* **Optimizer:** `AdamW`
* **Loss Function:** `CrossEntropyLoss`
* **Scheduler:** `LinearLR` warmup + `CosineAnnealingWarmRestarts`
* **Metrics:** `Weighted F1-score` via `TorchMetrics`
* **Early Stopping:** monitors validation F1, saves best model
* **Transforms:**

  * **Train:** `RandomResizedCrop`, `Flip`, `RGBShift`, `CoarseDropout`, `Blur`, `Normalize`
  * **Val/Test:** `Resize`, `CenterCrop`, `Normalize`

---

## **ğŸ“Š Results & Checkpoints**

* **Console Logs:** Epoch-wise loss, F1-score, LR.
* **W\&B Dashboard:** Full logs, metrics, system stats.
* **Saved Models:** In `checkpoints/` with run name and `_best.pth` suffix.

---

## **ğŸ“Œ To-Do**

* [ ] Change the dataset or make it a semantic segmentation task

---

## **ğŸ“„ License**

MIT License â€” see the `LICENSE` file for details.

---

## **ğŸ™Œ Acknowledgments**

* **Dataset:** ["A Large Scale Fish Dataset"](https://www.kaggle.com/datasets/crowww/a-large-scale-fish-dataset)
* **Libraries:** PyTorch, Hydra, Weights & Biases, Albumentations, TorchMetrics
* **Inspiration:** Modern deep learning pipelines and best practices